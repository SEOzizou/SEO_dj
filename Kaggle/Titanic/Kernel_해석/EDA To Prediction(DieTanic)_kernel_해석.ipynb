{"cells":[{"metadata":{"_uuid":"0ce8368deb46a77d2115dbc3d37859e8aa20c953","_cell_guid":"c986a9fb-e199-40eb-947d-c398d54f6b1e"},"cell_type":"markdown","source":"# EDA To Prediction (DieTanic) 커널 해석\n"},{"metadata":{"_uuid":"6255979b94583a7f5a069ef5dd551491c9df01f0","_cell_guid":"33cace66-52e3-4466-929c-6a506e6483ba"},"cell_type":"markdown","source":"### *Sometimes life has a cruel sense of humor, giving you the thing you always wanted at the worst time possible.*  \n-Lisa Kleypas\n\n                                                                                                                                     "},{"metadata":{"_uuid":"925765e573c2665df48f766467ed75eaab81190c","_cell_guid":"0bef0e9b-81e0-4737-b972-9cb8a06b6b63"},"cell_type":"markdown","source":"타이타닉의 침몰은 역사상 가장 악명 높은 침몰중 하나이다. 1912년 4월 15일에 첫 항해에서, 타이타닉은 빙산과 충돌한 뒤 가라앉았고, 2224명의 승객중 1502명의 승객이 사망하였다. 이것이 **DieTanic**라고 명칭한 이유이다. 이것은 전 세계에서 누구도 잊을 수 없는 재해이다.\n\n타이타닉을 만드는데에는 약 750만 달러가 쓰였고 충돌때문에 바다 아래로 가라앉았다. 타이타닉 데이터셋은 데이터 과학을 시작하고 캐글 컴피티션에 참가하려는 초보자에게 아주 좋은 데이터 셋이다.\n\n이 notebook의 목적은 예측 모델링 문제에서 작업흐름이 어떻게게 흘러가는지에 대한 아이디어를 제공해준다. feature를 확인하는 방법, 새로운 features를 더하는 방법, 그리고 몇개의 Machine Learning 개념들.... 나는 새로운 초보자들도 그것의 흐름을 이해할수 있도록 notebook을 가능한 기본적인 수준으로 만들려고 노력했다.  \n\n만약 당신이 notebook 이 좋고 이것이 도움이 됬다면 **PLEASE UPVOTE**. 이것은 글쓴이에게 큰 동기부여가 됩니다."},{"metadata":{"_uuid":"8fa2571b91b93e0b0a08b7a9e4eedc060ba76c20","_cell_guid":"706b0b7c-19f4-41c9-865f-5b3375253e0a"},"cell_type":"markdown","source":"## Contents of the Notebook:\n\n#### Part1: 탐색적 데이터 분석Exploratory Data Analysis(EDA):\n1)feature 분석(Analysis of the features).\n\n2)다양한 feature를 통해 관계나 trends 찾기\n(Finding any relations or trends considering multiple features.)\n\n#### Part2: Feature Engineering and Data Cleaning:\n1)새로운 features를 더하기(Adding any few features).\n\n2)불필요한 feature 제거(Removing redundant features).\n\n3)modeling을 위해 feature를 적절한 형태로 변환\n(Converting features into suitable form for modeling).\n\n#### Part3: Predictive Modeling\n\n1)기본적인 알고리즘(Running Basic Algorithms).\n\n2)교차검증(Cross Validation).\n\n3)앙상블(Ensembling).\n\n4)중요한 feature 추출(Important Features Extraction)."},{"metadata":{"_uuid":"18ba4a8f0909fd0a758b8cb8717327de8aeacdc8","_cell_guid":"bf5980c3-b168-4a26-81f3-c7bdcedb6429"},"cell_type":"markdown","source":"## Part1: Exploratory Data Analysis(EDA)"},{"metadata":{"_uuid":"7bb401b4e2e509cc8a53e9cf645226fa508fa2e2","_cell_guid":"d7601bd6-d22f-499f-97b9-85e01d390f05","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = df_test.PassengerId","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc88acccbc14bf28e4e4d49eee1ea82dcc31ab4","_cell_guid":"c12ac199-e3e3-4372-a747-baa6273f4561","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.insert(loc = 1, column = 'Survived', value = np.repeat(np.nan, df_test.shape[0], axis=0), )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data, df_test], axis = 0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1496190095fb1cd2e289c63c986c6eb951046860","_cell_guid":"5ef569cd-e99e-42f0-93ec-abbc6a90c00e","trusted":true},"cell_type":"code","source":"data.isnull().sum() #각 column 별 결측치 확인","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcedf70d9bdab89fb7eb8ff3769b14f0b5036a33","_cell_guid":"31972ab9-edef-49e3-bcd5-120262cf00d8"},"cell_type":"markdown","source":"**Age, Cabin and Embarked** 는 결측치를 가진다. 이것들을 수정할 것이다."},{"metadata":{"_uuid":"d433fbf891d9268f60bf395d7db4e61996989d04","_cell_guid":"841dc40d-06b4-4010-b996-8d1e23857341"},"cell_type":"markdown","source":"### 얼마나 많은 사람이 살아남았나??"},{"metadata":{"_uuid":"c60257aef24e867113873729829c7a1e33f4a0ab","_cell_guid":"fabb7625-a8ef-4f37-99c6-3ec93679ef1f","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True) # explode: pie 조각 떨어지는 정도 #autopct % 표시 형식\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11fa0236baf6d291f1a1f9325cc42d71fc1b61c1","_cell_guid":"5be22cbd-9b03-4e9f-8eaf-068dd9df401f"},"cell_type":"markdown","source":"많은 승객이 사고에서 살아남지 못한것은 명확하다.\n\ntraining set의 891명의 승객중 오직 약 350명정도가 살아남았다. 즉 오직 전체 training set의 **38.4%** 만 충돌에서 살아남았다. 우리는 데이터로부터 더 나은 통찰을 얻을 필요가 있으며 승객의 어떤 categories가 살아남거나 살아남지 못했는지 볼 필요가 있다.\n\n데이터셋의 다른 feature들을 사용해서 생존률을 확인할것이다. 이 feature들은 Sex, Port of Embarcation, Age, 등등이다.\n\n우선 feature 다른 타입들을 살펴 보자."},{"metadata":{"_uuid":"01e521761a33c3bc2961e2613f729c164269ee51","_cell_guid":"f16f40df-3681-4330-ba57-2955094a6546"},"cell_type":"markdown","source":"## Types Of Features\n\n### Categorical Features:\nA categorical variable 은 2개 이상의 범주를 가지고 있고 각각의 값은 범주화 될 수 있다. 예로,  성별은 categorical Feature로 2개의 범주를 가지고있다 (male and female). 이러한 값에 대해 어떤 순서를 매기거나 정렬을 할 수 없다. 이런 변수들은 **Nominal Variables(명목형 변수)** 로 알려져있다.\n\n**Categorical Features in the dataset: Sex,Embarked.**\n\n### Ordinal Features:\nordinal variable 는 categorical values와 유사하나 ordinal variable는 값들 사이에 상대적인 순서를 가지거나 정렬을 할 수 있다는 것이다. 즉 만약 **Tall, Medium, Short**의 값을 가지는 **Height**와 같은 feature 가 있다면. Height는 ordinal variable이다. 여기서 변수사이의 상대적인 순서가 있는것을 알 수 있다.\n\n**Ordinal Features in the dataset: PClass**\n\n### Continous Feature:\nfeature가 연속적이란것은 그것이 feature column의 최소값과 최대값 사이 또는 어떠한 두 값의 사이에 값을 취할 수 있다는것을 의미한다.\n\n**Continous Features in the dataset: Age**"},{"metadata":{"_uuid":"2b36f7862279cf64a76a9950f703bfed4ca220f6","_cell_guid":"ccd13018-e5fb-4022-ac41-cadce1994dbe"},"cell_type":"markdown","source":"## Analysing The Features"},{"metadata":{"_uuid":"8b5ad1ae98e4aad980f24bbefb489e6ac049768b","_cell_guid":"8d5bd219-61ce-4c88-b0c5-aaffce8cb1cc"},"cell_type":"markdown","source":"## Sex--> Categorical Feature"},{"metadata":{"_uuid":"3554e468c8581316a717348689f1d867b3c97f6a","_cell_guid":"428c84fc-9d5e-4022-a9f5-1c8ec7257268","trusted":true},"cell_type":"code","source":"data.groupby(['Sex','Survived'])['Survived'].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06e043424e13b87fcb020322e4869430fd0714f","_cell_guid":"06218a7d-bf3c-40b1-9cfa-2a915f7bc005","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebf3e75bbc120054b947162e088876bbaca3cf54","_cell_guid":"d97e7fe8-a98c-40f3-98c7-b5118c0295db"},"cell_type":"markdown","source":"이것은 꽤 흥미롭다. 배에 승선한 남자의 수는 여자의 수보다 더 많다. 그러나 생존한 여자의 수는 생존한 남자의 수의 거의 두배이다. 배의 승선한 **여자의 생존률은 거의 75%인 반면에 남자는 약 18~19%이다.** \n\n이것은 modeling에 **매우 중요한** feature로 보인다. 그러나 이것이 가장 좋은 것일까? \n<br>다른 feature들을 확인해보자."},{"metadata":{"_uuid":"e3b6327723dedd766d452f55b4056bbd0b37bed2","_cell_guid":"a210b0c8-dd8e-4fd0-a7f2-e597b7ed81e6"},"cell_type":"markdown","source":"## Pclass --> Ordinal Feature"},{"metadata":{"_uuid":"4a98fe27c4474296c6b51f4a2b7fb076c228b4b8","_cell_guid":"2477b536-32dd-43a0-8824-be034104b760","trusted":true},"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived, margins = True).style.background_gradient(cmap = 'terrain')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"592a3d8c24761c3f6e8c5cf875554826d9e308a5","_cell_guid":"c3adaaa2-f675-4273-ba93-8b26c37bacf3","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\n#sns.barplot('Pclass', y=\"Survived\", data=data)\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1]) #글쓴이는 수로 확인 저는 평균확인\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace6b99b7d75b76ecead6f03372c1790fe3aa6c7","collapsed":true,"_cell_guid":"eec00d94-07d2-4e6e-8797-cb6f98341793"},"cell_type":"markdown","source":"사람들은 **돈으로 모든것을 살 수 없다** 라고 말한다. 그러나 Pclass 1 의 승객들이 우선적으로 구조된것을 명확하게 확인 할 수 있었다. Pclass3의 승객이 훨씬 더 많긴 하지만 살아남은 수는 매우 낮다. 약 **25%**.\n\nPclass 1 의 생존률은 약 **63%** 이지만 Pclass 2의 생존률은 약 **48%** 이다. 이래서 돈과 직위는 중요하다. 더러운 물질만능주의 세상(의역 ㅎㅎ)\n\n조금 더 파해쳐보고 다른 흥미로운 관측치들을 확인하자. 한 번 **Sex와 Pclass**의 생존률을 한번에 보자."},{"metadata":{"_uuid":"1308ec5a68849984dfd1e05b53c52b6192363a18","_cell_guid":"7d413d16-2861-4aca-9042-38e374eddef3","trusted":true},"cell_type":"code","source":"pd.crosstab([data.Sex,data.Survived],data.Pclass, margins = True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710111beaace27f0e85958a0639f2b2175b0892c","_cell_guid":"1fd41001-f153-4a78-806b-72b16a34f88f","trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dbe07636bbd6d44fb0854c9c9cec03529117042","_cell_guid":"7ae5251a-8bd9-4638-85c8-b0e0ed91420e"},"cell_type":"markdown","source":"categorical values 쉽게 나눠 주기 때문에 여기서는 **FactorPlot** 를 사용하였다.\n\n**CrossTab** 과 **FactorPlot**를 보면 **Women from Pclass1**의 생존률은 약 **95-96%** 되는것을 확인할 수 있다. 오직  Pclass1의 여자 94명중 3명만이 사망하였다. \n\nPclass와는 상관없이 여자가 먼저 구조된것을 확인 할 수 있었다. 심지어 Pclass1의 남자도 매우 낮은 생존률을 보인다.\n\nPclass도 중요한 변수처럼 보인다. 이제 다른 feature을 분석해보자."},{"metadata":{"_uuid":"da1710d88cdb726d4c74d1580eb4650823f8e1a9","_cell_guid":"b9a8739f-9bfa-48a0-8b55-85694f8c7b36"},"cell_type":"markdown","source":"## Age--> Continous Feature\n"},{"metadata":{"_uuid":"58e1110e104a4628f2852fa284905525997c2c44","_cell_guid":"d8c1dc5a-2f74-4c88-9101-6c98abaf9878","trusted":true},"cell_type":"code","source":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2, figsize = (18,8))\nsns.violinplot('Pclass', 'Age', hue = \"Survived\", data = data, split=True, ax= ax[0])\nax[0].set_title(\"Pclass and Age vs Survived\")\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\", 'Age', hue = \"Survived\", data = data, split = True, ax = ax[1])\nax[1].set_title(\"Sex and Age vs Survived\")\nax[1].set_yticks(range(0,110,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c3ffbed55b33a51666b95a46c9f8aaa634bb73","_cell_guid":"b6f98f68-46c3-4aac-a4c7-ef350bf8ccd7"},"cell_type":"markdown","source":"#### Observations:\n\n1)Pclass에 따라 아이들의 수는 증가하고 10살 이하의 승객들의 생존률은 Pclass와는 상관없어 보인다.\n\n2)Pclass1의 20-50대 승객들을 높은 생존확률을 보이고 심지어 여자보다도 높다.\n\n3)남성들은 나이가 많아짐에 따라 생존확률은 감소한다.\nFor males, the survival chances decreases with an increase in age."},{"metadata":{"_uuid":"d347a55f54b1ee15ce39ad0e22d873a2c32fc736","_cell_guid":"23be39a5-be98-422b-8116-90cb0fd120ba"},"cell_type":"markdown","source":"사전에 봤던것처럼, Age feature는 **177** 개의 결측치를 가지고있다. 이를 대체하기 위해서 data set의 평균 나이를 할당할 것이다.\n\n그러나 문제는 나이가 다른 많은 사람들이 있다는 것이다. 4살짜리 아이에게 평균나이인 29살을 할당 할 수 없기에 승객이 어떤 나이대에 속할지 다른 방법은 없는지 생각해보자.\n\n**Bingo!!!!**, 여기서 **Name** feature를 확인 할 수 있다. 이 feature를 살펴보면 name에는 Mr or Mrs 와 같은 a salutation 을 가지고 있는것을 볼 수 있다. 그러므로 각각의 그룹의 Mr 그리고 Mrs의 평균값을 할당할 것이다.\n\n**''What's In A Name??''**---> **Feature**  :p"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Name","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e92f7a0ef7ea07abb81e602fc700b382c64fca96","_cell_guid":"0af8b99e-0d4b-4844-a146-da2a359ca2ff","trusted":true},"cell_type":"code","source":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    # '([A-Za-z]+)\\.' 정규표현식","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413c7cdc7469c86bd9d13fd54c731ecb58704197","_cell_guid":"8efd06d8-dee6-4892-a986-25944cf2bf61"},"cell_type":"markdown","source":"여기서 Regex(정규표현식): : **[A-Za-z]+)\\.**. 을 사용하였다. 그것이 하는것은 사이에 있는 문자열을 찾고 그 뒤를 **.(dot)** 가 뒤 따른다. 완벽하게 Name으로 부터 Initials를 추출해 냈다."},{"metadata":{"_uuid":"cf32ee39ef64840facd833f7f5d7616ffbaaa97c","_cell_guid":"e87e0415-43e7-4717-a6dc-43e60e71460e","trusted":true},"cell_type":"code","source":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0514c0b00c34f72bd77eb597a4e08a1c9edf982","_cell_guid":"1d7d94e4-240e-4cfc-ba21-d036b3bd7869"},"cell_type":"markdown","source":"잘목 적힌것 같은 Mlle 그리고 Mme가 있었는데 이건 Miss를 말하는것 같다. 이것들을 Miss 로 대체하고 똗같은 것들을 다른 값으로 대체할것이다. "},{"metadata":{"_uuid":"99a86205c88ad2c8fd96fc18225cd10ed91620dd","_cell_guid":"55b6028e-948c-4a98-a214-e86212481af4","trusted":true},"cell_type":"code","source":"data['Initial'].replace([\"Mlle\",\"Mme\",'Ms',\"Dr\",\"Major\",\"Lady\",\"Countess\",\"Jonkheer\",'Col',\"Rev\",'Capt',\"Sir\",\"Don\", 'Dona'],\n                        [\"Miss\",\"Miss\",\"Miss\", \"Mr\", 'Mr', \"Mrs\",\"Mrs\", \"Other\", \"Other\", 'Other', 'Mr', 'Mr', 'Mr', 'Other'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09f5aefc80a89f0f3af69cd9fdf30afb576f456","_cell_guid":"c1d0c1dd-ac10-4360-99e1-dfea7418ad0e","trusted":true},"cell_type":"code","source":"data.groupby('Initial')['Age'].mean() #lets check the average age by Initials","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2bbefba6442fcd47e04c90daa43f58fc001e47b","_cell_guid":"57ec5300-f0e3-46ce-a920-6c1846901b6d"},"cell_type":"markdown","source":"### Filling NaN Ages"},{"metadata":{"_uuid":"8bd3c34f7f539bc3d4720531da6405e2d0e96b46","_cell_guid":"f006b4b0-a8aa-432c-9bdb-040a435e77f8","trusted":true},"cell_type":"code","source":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd9e749a4eefeb0b57c0fe97de9b0ee9815c279","_cell_guid":"534ab487-2e49-4df1-93b4-4aac64c52bc1","trusted":true},"cell_type":"code","source":"data.Age.isnull().any() #So no null values left finally ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff9fcc871c21b4949d0082f3609151bb6f3e726","_cell_guid":"b2ed1983-50d5-405c-8bad-c61a087758f5","trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2, figsize = (20,10))\ndata[data['Survived']== 0].Age.plot.hist(ax = ax[0], bins = 20, edgecolor = 'black', color = 'red')\nax[0].set_title('Survived = 0')\nx1 = list(range(0, 85, 5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1], color = 'green', bins = 20, edgecolor = 'black')\nax[1].set_title('Survived = 1')\nx2 = list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e78651053e3b3641da3ded5bac87be689c7df259","_cell_guid":"b4c65724-4641-4e8c-be59-c9d41bfd088b"},"cell_type":"markdown","source":"### Observations:\n1)갓난아기(AGE<5)는 대다수가 살아남았다(The Women and Child First Policy).\n\n2)가장 나이많은 승객은 살아남았다(80세)\n\n3)사망의 가장많은 수는 30~40대의 그룹에 속한다."},{"metadata":{"_uuid":"e81923d749fef3cfc374b9b2dcbc9f27e8cc1ecc","_cell_guid":"82ec9949-9681-42d0-959d-02fe4ff2675c","trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', col = 'Initial', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0486c4864f225d54bbc392fa01ec068e9d89d5","_cell_guid":"2e547f77-4b30-4e76-baf8-1ece2f74074d"},"cell_type":"markdown","source":"class에 상관없이 여성과 아이를 우선적으로 구했다."},{"metadata":{},"cell_type":"markdown","source":"## Embarked--> Categorical Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([data.Embarked,data.Pclass],[data.Sex, data.Survived], margins= True).style.background_gradient(cmap=\"summer_r\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chances for Survival by Port Of Embarkation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived',data =data)\nfig=plt.gcf() #figure 에 접근\nfig.set_size_inches(5,3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"port C에 대한 생존확률이 0.55로 가장 높고 S에서 가장 낮다."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked', data = data, ax = ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked', hue = 'Sex', data = data, ax = ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue = 'Survived', data =data, ax = ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue = 'Pclass', data=data, ax = ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5) # 그래프간 간격 조정 width height\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n1)다수의 승객이 S에서 탑승했다. 그들 중 다수가 Pclass3 이다.\n\n2)C에서 탑승한 승객들은 생존률을 보면 상당히 운이 좋은것 처럼 보인다. 이에 대한 이유는 아마도 Pclass1 과 Pclass2의 모든(다수) 승객들이 구조되서 그럴것이다.\n\n3)The Embark S 대다수의 부자 사람들이 탑승한 port처럼 보인다. 그러나 생존률은 여전히 낮은데 아마도 약 **81%** Pclass3 사람들이 살아남지 못했기 때문이다.\n\n4)Port Q는 거의 95%의 승객이 Pclass3로부터 왔다."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Sex', col = 'Embarked', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n1)Embarked와 상관없이 Pclass1 과 Pclass2 여성의 생존확률은 거의 1이다.\n\n2)여성과 남성 모두 생존률이 매우 낮기 때문에 port S는 Pclass3 승객에게는 매우 불행한것 처럼 보인다.**(Money Matters)**\n\n3)port Q는 남자에게 매우 불행한것처럼 보인다. 왜냐면 거의 모든사람이 Pclass3출신이기 때문이다."},{"metadata":{},"cell_type":"markdown","source":"### Filling Embarked NaN\n\n제일많은 승객들이 port S에서 탑승했기 때문에 NaN값을 S로 대체한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Embarked'].fillna('S',inplace=True)\n\ndata.Embarked.isnull().any()# 마침내 결측치가 존재하지 않는다.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SibSip-->Discrete Feature\n이 feature는 홀로 사는지 아니면 가족과 같이 사는지를 나타낸다.\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Survived).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2, figsize = (20, 8))\nsns.barplot('SibSp', 'Survived',data =data, ax = ax[0])\nax[0].set_title('SibSp vs Surivved')\nsns.factorplot('SibSp', 'Survived', data = data, ax = ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n\nThe barplot and factorplot에 의하면 만약 승객이 sibling 없이 혼자 탑승했다면 34.5%의 생존률을 보였다. 그래프는 대략 sibling의 수가 증가함에 따라 생존률이 감소한다. 이건 이해가 된다. 즉 한 사람이 가족과 함께 탑승 했다면 내 가족을 구하기 위해 자신을 희생할 것이다. 놀랍게도 5-8명의 가족들의 생존률은 **0%** 보인다. 이유는 아마도 Pclass일겁니다???\n\n이유는 **Pclass** 이다. crosstab을 보면 SibSo>3 인 사람들은 모두 Pclass3dp 속해있다. Pclass3에 있는 대 가족들은 모두 사망했다는것이 분명하다."},{"metadata":{},"cell_type":"markdown","source":"## Parch"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"다시 crosstab에 의하면 대가족들은 Pclass3에 속한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n여기 매우 유사한 결과들이 보인다. 부모와 함께 탑승한 승객들은 높은 생존확률을 보였다. 그러나 수가 커질수록 생존률이 감소하였다.\n\n생존률은 1-3 parents가 배에 탑승한 사람들이 높았다. 혼자 있다는것은 치명적이고 >4 parents를 가진 사람들은 생존률이 감소하였다."},{"metadata":{},"cell_type":"markdown","source":"## Fare--> Continous Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Highest Fare was:',data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Fare.fillna(data.groupby(['Pclass', 'Sex']).Fare.transform('median'), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"가장 낮은 탑승요금은 **0.0**입니다. Wow!! a free luxorious ride. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare, ax =ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare, ax = ax[1])\nax[1].set_title('Fare in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pclass1 의 승객의 요금에는 a large distribution 이 있는게 보인다 그리고 이 distribution은 standards 가 감소함에 따라 감소한다. Fare또한 연속형이기 떄문에 binning을 통해 discrite하게 만들 수 있다."},{"metadata":{},"cell_type":"markdown","source":"## Observations in a Nutshell for all features:\n**Sex:** 남성과 비교하면 여성의 생존률이 높다.\n\n**Pclass:**1st calss 의 승객들이 더 높은 생존률을 보였다. Pclass3 승객들의 생존률을 매우 낮았다. **여성들의 경우에**, **Pclass1** 의 승객들의 생존률은 거의 1에 가까웠고 **Pclass2** 승객들 또한 매우 높았다. **돈이 최고다!!!**.\n\n**Age:** 5-10살의 아이들은 매우 높은 생존률을 보였다. 15-35세 사이의 승객들은 많이 사망했다.\n\n**Embarked:** 이건 매우 흥미로운 feature이다. **대다수의 Pclass1 의 승객이 S에서 탑승했음에도  C에서의 생존률은 좋아 보인다.** Q에서의 승객들은 모두 **Pclass3**이다.\n\n**Parch+SibSp:** 1-2명의 형제 자매나 배우자 또는 1-3명의 부모님과 같이 탑승한 승객은 혼자 탑승하거나 큰 가족 단위로 여행하는 승객보다 더 높은 생존률을 보인다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Correlation Between The Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(), annot=True, cmap = 'RdYlGn', linewidth = 0.2)#data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Interpreting The Heatmap\n\n먼저 주목할 점은 알파벳이나 문자열은 상관계수를 구할 수 없는것이 명백하기 때문에 숫자 feature들만 비교됬다는 것이다. plot을 이해하기전에 correlation이 정확히 뭔지 한 번 보자.\n\n**POSITIVE CORRELATION:** 만약 **feature A의 증가가 feature B의 증가로 이어진다면 두 변수 사이는 positively correlated** 이다. 값이 **1이라는 건 완벽한 양의 상관관계를 의미한다**.\n\n**NEGATIVE CORRELATION:** 만약 **feature A의 증가가 Feature B의 감소로 이어진다면 두 변수는 negatively correlated 되어있다**. 값이 **-1이라는건 완벽한 음의 상관관계라는것을 의미한다. (negative correlation)**.\n\n2개의 features가 높게 correlated 되어있다면 한 feature 증가는 다른 feature 증가로 이어질것이다. 이것은 2개의 feature 가 유사한 정보를 담고있고 매우 적은 정보안에 분산을 가지고 있다는 것을 의미한다. 이것은 **다중공선성** 으로 알려져 있으며 그 두개의 변수가 거의 동일한 정보를 가지고있는 것이다.\n\n그러면 우리가 그것들중 하나의 변수가 쓸모없음에도 그 두변수 모두 사용해야할까? 트레이닝을 만드는 동안 우리는 그 불필요한 feature를 제거한후에 그것이 trainin time을 감소시키고 많은 이점이 있는지 확인해야 한다.\n\n위의 heatmap을 보건데 feature간에 상관관계는 많지 않은것을 볼 수 있다. 가장 높은 correlationdms **SibSp 와 Parch 즉 0.41** 정도이므로 우리는 모든 feature를 사용할 것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Part2: Feature Engineering and Data Cleaning\n\nFeature Engineering이란 무엇일까?\n\n우리가 feature와 dataset이 주어졌을때 모든 feature가 중요하지는 않을것이다. 아마도 제거되어야 할 불필요한 features들이 존재할 것이다. 또한 다른 feature로 부터 정보를 얻어서 새로운 feature를 생성할 수 있다.\n\n한 예시로는 Initial feature를 생성한것이다 Name feature를 사용하여서. 우리가 새로운 feature를 얻거나 제거할 수 있는지 확인해 보자. 또한 존재하는 연관된 feature를 modeling에 적합한 형태로 바꿀것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Age_band\n\n#### Problem With Age Feature:\n앞서 언급하였다 싶이 **Age는 연속형 feature**인데 머신러닝 모델에 연속형 변수와 관련한 문제가 있다.\n\n**Eg:**만약 내가 운동 선수를 group을 만들거나 정렬을 해야한다면 **성별** 을 이용할 수 있다. 즉 남성과 여성으로 쉽게 구분지을 수 있다.\n\n만약 이제 **나이** 로 그들을 그룹을 지어야 한다면, 어떻게 할 것인가? 만약 30명의 사람이 있다면 30개의 나이값이 있을것이다. 이제 이건 문제를 야기시킨다.\n\n우리는 이제 이 **연속형변수를 binning과 nomalisation을 통해 범주형 변수로** 바꿔줄 필요가 있다. 여기서는 binning 즉 age 범위를 그룹화 시켜주는것을 통해 하나의 구간 또는 그것들의 하나의 값을 할당해줄 것이다.\n\n승객의 최대 나이는 80세이니 따라서 0-80나의를 5개의 구간으로 나누자. \n즉 80/5 = 16이므로 구간의 크기는 16이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Age_band','Survived',data=data,col='Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Pclass와 상관없이 나이가 증가함에 따라 생존률이 감소하는것을 볼 수 있다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Family_Size and Alone\n이시점에서 \"Family_size\"와 \"Alone\"이라는 새로운 feature만들어 그것을 분석 할 수 있다. 이 Feature는 Parcg와 SibSp의 합이다. 이건 combined(합쳐진?) Data를 제공해서 생존률이 승객의 가족의 크기에 영향이 있는지 확인 할 수 있게 한다. Alone은 승객이 혼자인지 아닌지를 말해주는 것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n\nf,ax = plt.subplots(1,2, figsize=(18,6))\nsns.factorplot('Family_Size', 'Survived', data = data, ax =ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.factorplot('Alone', 'Survived', data = data, ax = ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Family_Size=0 건 승객이 혼자 탑승했다는것을 의미한다.** 명확하게 만약 혹자 탑승했거나 family_size=0이라면 생존확률이 매우 낮다. Family_size>4인 사람들 또한 매우 낮은 생존률을 보인다. 이것 또한 model에서 중요한 feature라고 생각되어진다. 조금 더 조사해보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Alone','Survived', hue = 'Sex', col = 'Pclass', data = data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"혼자인 여성의 생존률이 가족이 있는 여성보다 더 높은 Pclass3를 제외하면 성별이나 Pclass 를 막론하고 혼자있다는것은 매우 낮은 생존률을 보인다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Fare_Range\n\n요금은 중요한 연속형 feature이기 때문에, 우리는 이것을 ordinal 값으로 바꿔야한다. 이것을 위해 **pandas.qcut** 을 사용할 것이다.\n\n**qcut**은 입력한 구간의 값에 따라 값을 나누고 정렬한다. 만약 우리가 5개의 구간을 입력했다면 그것은 동일한 크기의 구간으로 값을 정렬시킬것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"위에서 논의되었던 것처럼, **fare_range가 올라감에따라 생존률도 올라가는것** 을 확인할 수 있다.\n\nFare_Range 값을 그대로 입력할 수 없으니 **Age_Band**에서 했던것처럼 구간을 하나의 값으로 바꿔줄 것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[(data.Fare<=7.91),'Fare_cat'] = 0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat'] = 1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat'] = 2\ndata.loc[(data['Fare']>31),'Fare_cat'] = 3\n\nsns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"명확하게 Fare_cat이 증가함에따라 생존률도 증가하는것이 보인다. 이 feature는 Sex와 함께 모델링 할때 중요한feature될것이다.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Converting String Values into Numeric\n\n우리는 머신러닝 model에 문자열을 전달 할 수 없기에 Sex, Embarked와 같은 feature를 numeric value로 바꿔줘야한다."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Sex'].replace(['male','female'],[0,1],inplace = True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2], inplace = True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4], inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Dropping UnNeeded Features\n\n**Name**--> 이미 범주형변수(categorical value)값으로 변환하였기 때문에 더이상 name feature는 필요없다.\n\n**Age**--> Age_band feature가 있으므로 이것도 더이상 필요없다.\n\n**Ticket**--> 이것은 categorised(범주화)될수 없는 무작위 문자이다.\n\n**Fare**--> Fare_cat feature를 가지고 있으므로 필요없다.\n\n**Cabin**--> 많은 결측값과 또한 많은 승객이 다수의 cabin을 가지고 있으므로 이것또한 필요없는 변수이다.\n\n**Fare_Range**--> fare_cat feature가 있다.\n\n**PassengerId**--> 범주화 될수 없다.."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Fare_Range', 'PassengerId'], axis = 1, inplace =True)\nsns.heatmap(data.corr(),annot = True, cmap = 'RdYlGn', linewidths = 0.2, annot_kws = {'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"위에 correlation plot을 보면 우리는 몇개의 양의 상관관계를 가진 feature들을 볼 수 있다. 양의 상관관계는 **SibSp andd Family_Size** and **Parch and Family_Size** 이고 음의 상관관계는 **Alone and Family_Size.** 이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = data[data['Survived'].isnull()].drop('Survived', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna(subset = ['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"# Part3: Predictive Modeling\n\nEDA를 통해 insights를 얻었다. 그러나 그것만으로는 정확하게 어떤 승객이 살아남을지 예측할 수는 없다. 그래서  훌륭한 classification Algorithms을 사용해서 승객의 생존여부를 예측할 것이다. 다음은 모델을 만들기위해 사용한 알고리즘이다 :\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Radial Support Vector Machines(rbf-SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Support Vector Machine(linear-SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### K-Nearest Neighbours(KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"KNN 모델에대한 정확도는 **n_neighbours**의 값을 바꿀때마다 따라 달라진다. 초기값은 **5** 로 설정 돼있다.\nn_neighbours에 따른 정확도를 확인해보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"모델의 정확도만 classifier의 robustness(안정도?)를 결정하는 것은 아니다. classifer 가 traing 되고 test 했을때 정확도가 90%를 기록했다고 하자.\n\n이것은 성능좋은 classifier로 보이지만, 그러나 새로운 test set에 test됬을때 정확도가 90%거라고 말할 수 있을까? 답은 **그렇지 않다** 이다, 왜냐면 우리는 classifier가 어떤 instance를 사용해서 학습될지 결정 할 수 없기 때문이다. 트레에닝 data와 test데이터가 바뀜에 따라 정확도 또한 달라질것이다. 그것은 증가할 수도 또는 감소할 수도 있다. 이것은 **model variance** 라고 알려져 있다.\n\n이것을 극복하기위해 좀더 일반화된 model을 얻는데, **Cross Validation**을 사용한다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Cross Validation\n\n많은 경우에 데이터가 불균형하다. 즉 class1의 instance는 많이 존재하지만 다른 class의 수는 적다. 그러므로 우리는 반드시 데이터셋의 모든 instance에 train 하고 test하여야 한다. 그러면 우리는 데이터셋에 대하여명시된 정확도의 평균을 얻을 수 있다. \n\n1)The K-Fold Cross Validation은 k개의 subset(부분집합)으로 데이터셋을 나눈것으로 작동한다.\n\n2)데이터셋을 (k=5) 부분으로 나눈다고 하자. 우리는 한개의 부분을 테스트를 위해 남겨주독 4개의 부분을 알고리즘을 통해 학습시킨다.\n\n3)각각의 반복에서 테스트하는 부분을  바꾸고 나머지 부분으로 알고리즘을 training 함으로서 이 과정을 계속한다. 정확도와 오차를 평균을 내면 알고리즘의 평균적인 정확도를 알 수 있다.\n\n이것을 k-fold Cross Validation이라 부른다.\n\n4)한 알고리즘은 어떠한 training data에 대해서는 underfit 될 수 있고 때때로 다른 training data에 대해 overfit 될 수도있다. 그러므로 cross-validation을 통해 좀 더 일반적인 모델을 얻을 수 있다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"불균등성 때문에 classification accuracy는 때때로 잘못 해석(misleading)될 수도 있다. confution matrix를 통해 더 종합적인 결과를 얻을 수 있는데 이것이 모델이 어디로 잘못 갔는지 또는 어떤 class를 모델이 잘못 예측했는지 알려줄 것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Confusion Matrix\n\nclassifier에 의해 예측된 맞은 예측과 틀린 예측의 수를 보여준다."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Interpreting Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"왼쪽의 대각선은 각각의 class에 대하여 올바르게 예측한 경우의 수를 의미하고 오른쪽 대각선의 경우는 잘못 예측한 경우의 수이다. 첫번째 그래프인 rbf-SVM을 살펴보자:\n\n1)옳은 예측의 수는 **491(사망) + 247(생존)** 이고 평균적인 CV 정확도는 **(491+247)/891 = 82.8 %** 을 보였다.\n\n2)**Errors**-->  58명의 사망자들은 생존으로 95명의 생존자들은 사망했다고 잘못 예측되었다. 그러므로 살아남은 사람들을 사망헀다고 예측하는 잘못 예측하는 일이 더 많았다.\n\n모든 metrices을 보면 rbf-SVM이 가장 사망자를 잘예측하였으며 NaiveBayes의 경우 생존자를 가장 잘 예측하였다.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Hyper-Parameters Tuning"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"머신러닝 모델의 경우 블랙박스와 같다. 이 블랙박스의 경우 parameter의 초기값이 있는데 더 나은 모델을 얻기 위해 parameter를 tune 하고 change 하여야 한다. SVM 모델의 C 와 gamma 처럼 다른 classifiers에 대해 다른 parameter들이 존재하고 이것들을 hyper-parameter라고 부른다. hyper-parameter를 tune하고 알고리즘의 learning-rate를 바꾸면서 우리는 더 나은 모델을 얻을 수 있다. 이것은 Hyper-parameter Tuning 이라고 부른다.\n\n여기서는 2개의 가장 좋은 모델인 SVM과 RandomForest를 hyper - parameter tunning 할 것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators=range(100,1000,100)\nmax_depth = range(2,8)\nhyper={'n_estimators':n_estimators, 'max_depth' : max_depth}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,cv=5,verbose=True, n_jobs=4)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Rbf-SVM의 최고 score는 **82.82%고 C = 0.5 그리고 gamma = 0.1 이다.**\nRandomForest의 경우 **83.27%이고 n_estimators = 100 그리고 max_depth = 4 이다.**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Ensembling"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"앙상블은 model의 성능과 정확도를 올리기위한 좋은 방법이다. 간단하게 말하면 이것은 하나의 강력한 model을 만들기 위해 다양한 간단한 모델을 조합하는것을 의미한다. \n\n만약 우리가 핸드폰을 구매하려고하고 많은 사람들에게 다양한 조건에 기반하여 물어봤다고 하자. 다른 조건들을 분석한 후에 우리는 한 상품에 대해 더 확신에 찬 판단을 만들 수 있다. 이것이 **EnSembling** 이다. 앙상블은 다음과 같은 방법으로 된다:\n\n1)Voting Classifier\n\n2)Bagging\n\n3)Boosting."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Voting Classifier\n\n이것은 은 다른 간단한 머신러닝 모델로 부터의 예측을 조합하는 가장 간단한 방법이다. 이것은 기반이 되는 모든 모델의 예측에 근거하여 평균적인 예측결과를 제공한다. 기반이 되는 모델은 모두 다른 타입이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=100,random_state=0, max_depth = 4)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = ensemble_lin_rbf.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([\"cc\"=a, \"dasdaasd\"= a], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([test_id, pd.Series(a)], axis = 1).to_csv('SUBMIT.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Bagging\n\n배깅은 일반적인 앙상블 방법이다. 이것은 유사한 classifiers를 데이터셋의 작은 부분에 적용하고 모든 예측의 평균을 내는것이다. 평균을 내는 덕분에 분산을 감소시킨다. Voting Classifier과는 다르게 Bagging은 유사한 classifier들만 이용한다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Bagged KNN\n\n배깅은 높은 분산을 가진 모델에 가장 잘 작동한다. 이것의 예시로는 Decision Tree또는 Random Forests를 들 수 있다.\n여기서는 작은 **n_neighbours** 값을 가지는 KNN을 사용할 것이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Bagged DecisionTree"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Boosting\n\n부스팅은 연속적인 classifiere의 학습을 사용하는 앙상블이다. 이것은 약한 모델의 단계적인 강화과정으로 부스팅이 작동하는과정은 다음과 같다:\n\n모델은 우선 전체 데이터셋에 대하여 학습된다. 그러면 어떤 instance들은 올바르게 예측된 반면에 어떤것들은 잘못 예측됬을 것이다. 다음의 학습서 모델은 잘못 예측된 instance 에 집중하거나 또는 그것들에 대하여 더 가중치를 줄것이다. 그러면 잘못 예측된것들을 올바르게 예측하도록 시도 할것이다. 이제 이 과정을 계속해서 정확도의 한계에 다다를때 까지 새로운 classifier를 모델에 더할것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### AdaBoost(Adaptive Boosting)\n\n이 경우에 약한 learner or estimator(모델)은 Decision Tree 였으나 선택의 따라 초기 base_estimator을 바꿀 수 있다. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Stochastic Gradient Boosting\n\n여가도 또한 weak learner 로 Decision Tree를 사용하였다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"가장 높은 정확도는 AdaBoost 였고 Hyper-Parameter Tuning을 통해 정확도를 향상시킬것이다."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Hyper-Parameter Tuning for AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True, njob)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"AdaBoost를 이용해서 얻은 최적의 정확도는 **83.16% 이고 n_estimators=200 and learning_rate=0.05이다.**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Confusion Matrix for the Best Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"RandomForests, AdaBoost 등등.. 과 같은 다양한 classifiers의 feature importance를 확인 할 수 있다.\n\n#### Observations:\n\n1)일반적으로 중요한 features 는 Initial,Fare_cat,Pclass,Family_Size이다.\n\n2)Sex feature의 경우 어떠한 중요도를 보이지 않았다 이것은 충격적이다 왜냐면 우리는 전에 확인했기 때문이다 Pclss와 함께 Sex 는 확연한 차이를 나타내는 factor였기 때문이다. Sex는 오직 RandomForests에서만 중요한것처럼 보인다.\n\n그러나 feature Initial을 보면, 이건 많은 classifiers에서 순위권에 위치하고 있다. 이미 Sex 와 Initial 과의 양의 상관 관계를 확인했기 때문에 두개다 모두 성별을 의미한다고 할 수 있다.\n\n3)유사하게 Pclass 와 Fare_cat은 승객의 지위(?)를 나타내고 Family_Size 도 다른변수 Alone,Parch and SibSp과 유사하다.\n\n여러분 모두가 머신러닝에 관해 뭔가 알아갔기를 희망합니다. 머신러닝에 관한 다른 훌륭한 notebook 들은 다음과 같습니다.\n1) For R:[Divide and Conquer by Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82297/notebook)\n\n2)For Python:[Pytanic by Heads and Tails](https://www.kaggle.com/headsortails/pytanic)\n\n3)For Python:[Introduction to Ensembling/Stacking by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)\n\n### 이 글을 봐주셔서 감사합니다 . 만약 이 글이 도움이 되었다면 [https://www.kaggle.com/ash316/eda-to-prediction-dietanic]들어가셔서 **Upvote를 눌러주세요**\n\n### I'm really appreciate Writer of this original kernel. i've learn so many things and Those are so helpful for me. 그러니 이 한글번역이 도움위 되셨다면 링크 들어가셔서 Upvote 한 번만 눌러주세요 감사합니다."}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}