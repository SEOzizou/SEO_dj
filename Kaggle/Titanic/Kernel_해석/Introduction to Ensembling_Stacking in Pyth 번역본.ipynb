{"cells":[{"metadata":{"_uuid":"e919d1161f20999e599ba1fd66a5a45b9c82f229","_cell_guid":"bc64948f-5d6a-078d-085d-1beb58687bd3"},"cell_type":"markdown","source":"https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n위의 주소의 커널을 해석하였습니다. 이 글이 맘에 드신다면 위의 링크로 가셔서 upvote버튼을 눌러주세요.\n\n# Introduction\n\n\n이 노트는 기본 학습 모델을 앙상블 (결합)하는 방법, 특히 스태킹 (stacking)으로 알려진 앙상블의 변형에 대한 매우 기본적이고 간단한 소개 입문서입니다. \n간단히 말해서 스태킹은 첫 번째단계에서 몇 가지 기본 분류모델의 예측을 사용한다음 두 번째 level에서 다른 모델을 사용하여 이전 수준의 첫 번째 예측 결과를 예측하는 것입니다.\n\n타이타닉 데이터셋은 캐글을 접한지 얼마 안되는 초보자들에게 이 개념(Stacking)을 소개하는 좋은 데이터셋 중 하나입니다. 그러나 스태킹이 Kaggle 대회에서 많은 팀이 우승하도록 기여했음에도 불구하고 이 주제에 대한 커널이 부족한 것으로 보이므로 이 노트북이 그 공백을 조금이라도 채울 수 있기를 바랍니다.\n\n나(kernel Writer)도 Kaggle에 온지 얼마 안 된 신참이고 우연히 만나고 공부할 수 있던 ensembling / stacking script는 훌륭한 Faron에 의한 the AllState Severity Claims competition에 쓰여있던것이었다.(Open 되어 있지 않고 공부하기 힘들었다 이런뜻인듯). 이 노트북에 쓰여진 자료들은 Faron의 script에서 많이 차용되었습니다. classifier의 앙상블이 여기에서 쓰였지만 그는 regressors의 앙상블을 이용했습니다.\n어쨌든 그의 script를 한 번 확인하십시오:\n\n[Stacking Starter][1] : by Faron \n\n이 notebook을 앞에 두고 앙상블의 개념을 직관적이고 간결한 방식으로 다루고 전달할 수 있기를 희망합니다. 나의 다른 독립적인 Kaggle [script][2]는 아래에서 논의되는 앙상블 단계를(다른 매개 변수임에도 불구하고) 정확하게 동일하게 수행하는데, 이 Kaggle [script][2]은 top 9%에 들기에 충분한 0.808의 LB 점수를 기록하고 4분 안에 코드가 실행됩니다. 따라서 나는 그 스크립트를 개선하고 추가 할 여지가 많다는 것을 확신합니다. 어쨌든 내가 어떻게 개선 할 수 있는지에 대한 의견을 남겨주세요.\n\n [1]: https://www.kaggle.com/mmueller/allstate-claims-severity/stacking-starter/run/390867\n [2]: https://www.kaggle.com/arthurtok/titanic/simple-stacking-with-xgboost-0-808"},{"metadata":{"_execution_state":"idle","_cell_guid":"14630296-b1aa-759e-bafa-b6a73f3896ed","_uuid":"2e37a274400cfeb472b6405d524325245588dd66","trusted":true},"cell_type":"code","source":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\n#from sklearn.cross_validation import KFold\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b590aafe06a2ac55daae9d2456155e457914f5f","_cell_guid":"d647b74c-099b-851a-dcd2-3a58c9e8f10c"},"cell_type":"markdown","source":"# Feature Exploration, Engineering and Cleaning \n\n이제 대부분의 커널이 구성되어있는 구조와 매우 비슷하게 진행될것입니다. \n<br>즉 먼저 데이터를 탐색하고, feature engineering 가능성을 확인하고 범주형 features를 numrically(수치형)으로 바꿔줄것 입니다. "},{"metadata":{"_execution_state":"idle","_cell_guid":"5937fd72-d1ad-f678-cc82-f08a96e4cad0","_uuid":"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac","trusted":true},"cell_type":"code","source":"# train 과 test 데이터셋을 불러드립니다.\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#승객의 ID를 저장합니다 for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81378834770e55c76751347588322fe32acf5737","_cell_guid":"dff1c1dd-1e59-3907-88fa-1a1d699122be"},"cell_type":"markdown","source":"당연하게도 우리가 할일은 범주형 변수에서 정보를 얻어내는것 입니다.\n\n**Feature Engineering**\n\nfeature enginnering idea를 얻기 위해 매우 이해하기 쉽게 잘 알려주는 Sina's notebook에서 많은 것을 배웠습니다. 그러니 그의 작업을 한번 확인해보세요.\n\n[Titanic Best Working Classfier][1] : by Sina\n\n\n  [1]: https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier"},{"metadata":{"_execution_state":"idle","_cell_guid":"e85b2a80-88a4-928f-f2b2-24895dea38f3","_uuid":"b1c67249f91768ce8e5e2751364d32c87446cf55","trusted":true},"cell_type":"code","source":"full_data = [train, test]\n\n# 스스로 더한 몇몇의 feature\n# 이름의 길이를 제공해준다.\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# 타이타닉에 탄 승객이 Cabin의 유무에대한 feature\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Sina가 수행한 Feature engineering 단계들\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# 혼자인지 여부에 대한 새로운 feature 생성\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Embarked에 있는 결측치 모두 삭제\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Fare 열에 있는 결측치를 모두 삭제하고 새로운 CategoricalFare를 만든다.\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# 새로운 CategoricalAge를 생성한다.\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# 승객에서 title를 알아내기 위한 함수를 정의한다.\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # title이 존재한다면 그것을 추출하고 그 값을 반환한다.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# 승객의 title을 포함하고 있는 새로운 feature인 Title을 생성한다.\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# 흔하지 않은 title은 단일 그룹인 'Rare'로 포함시킨다.\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_cell_guid":"8fc645ba-4b38-cea7-17b3-02175cb103d9","_uuid":"ca2d48b03d45f914db2ee9ae3ee95aad8fb20431","trusted":true},"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd288776321804d99e4e4a7e88594c1d631e4409","_cell_guid":"e9814168-b7cd-d4e4-1b1d-e21c6637a663"},"cell_type":"markdown","source":"이제 features를 정리했고 적절한 정보들을 얻고 범주형 column들을 삭제하였습니다. 이제 모든 features은 머신러닝 모델에 적합한 형식인 수치형(numeric)이어야 합니다. 다음 단계로 진행하기 전에 간단한 상관관계와 변환된 데이터셋의 분포 plot을 그려보자.\n## Visualisations "},{"metadata":{"_execution_state":"idle","_cell_guid":"fc426b8f-873d-6f23-4299-99f174956cca","_uuid":"1f280a1c11dc35a93b57af494938998e6d0b4544","trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3544b03040a1d691f6c48433c84d1e57f3c15e3d","_cell_guid":"41102927-8218-415a-0b73-5129c8f5dd0c"},"cell_type":"markdown","source":"**Pearson Correlation Heatmap**\n\n몇개의 feature들의 상관관계 plot을 생성해서 한 feature과 다른 feature와 연관되어 있는지 확인해 볼것입니다. 그렇게 하기 위해 Seaborn Plotting package를 활용할것 입니다. 이 package는 다음과 같이 heatmap을 매우 편리하게 나타내어 줍니다."},{"metadata":{"_uuid":"b6142da58d9515979930abee19549aacf7f62c9f","_cell_guid":"af2eba12-b836-42a1-9ff3-d7a55bec8f9d","trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6c7a1ea3866d177016a1e13c5d9e7341c49c147","_cell_guid":"ccc92745-0680-df4d-d709-10003475d8e3"},"cell_type":"markdown","source":"**Takeaway from the Plots**\n\nthe Pearson Correlation plot 을 통해 알 수 있는것은 많은 변수들이 서로간에 큰 상관성을 보이지 않는다는 것입니다. 이건 이 feature들을 당신의 learning model에 feeding(활용? 넣어준다?) 관점에서는 좋은 현상입니다. 왜냐면 이건 우리의 학습 데이터에 불필요하거나 지나치게 많은 데이터가 많지 않다는것을 의미하고 각각의 feature가 unique한 정보를 가지고 있다는 것입니다. 여기서 가장 강한 상관성을 보이는 것은 Family size 와 Parch(Parents 와 children)입니다. 여기서는 연습을 위한 목적으로 두 개의 feature 다 남겨 둘 것입니다.\n.\n\n**Pairplots**\n\n마지막으로 하나의 피쳐에서 다른 피쳐로 데이터의 분포를 관찰하기 위해 몇 개의 pairplots을 생성하겠습니다. 다시 한번 이 작업을 위해 시본 (Seaborn)을 사용합니다."},{"metadata":{"_execution_state":"idle","_cell_guid":"ea6b0a8f-5a33-666f-8057-c0d689f370f5","_uuid":"624446543aafd518025fd3f5346d32ee1aab6f9a","trusted":true},"cell_type":"code","source":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74f445f5d9fb9622c4609c00679254abb3c91b1","_cell_guid":"32ac3ce1-42bc-9a7d-44f3-4b7c025a921c"},"cell_type":"markdown","source":"# Ensembling & Stacking models\n\n마침내 feature engineering 과 formatting에 관해 간단하고 매우 빠르게 진행된 과정을 통해, 우리는 이 notebook의 요지와 meat(목표?)에 도달했습니다.  \n\nStacking ensemble을 만듭시다!"},{"metadata":{"_uuid":"4e5fda8c6c92fc3cdf8e906089494f80e7c37245","_cell_guid":"3e922821-5a10-040b-305a-c2d47d633c49"},"cell_type":"markdown","source":"### Helpers via Python Classes\n\n여기에서는 파이썬 클래스를 사용하여보다 편리하게 사용할 수 있도록합니다. 프로그래밍을 처음 시작하는 사람은 일반적으로 객체 지향 프로그래밍 (OOP)과 함께 사용되는 클래스를 들어봤을것 입니다. 즉, 클래스는 객체(구시대 사람들을 위한 변수) 생성을위한 코드 / 프로그램 을 확장하고 해당 클래스에 특정한 함수 및 메소드를 구현하는 데 도움이됩니다.\n\n\n아래의 코드 섹션에서는 모든 Sklearn classifiers에 공통적으로 내장 된 메서드 (예 : train, predict and fit)를 확\n장 할 수있는 클래스 *SklearnHelper* 를 사용합니다. 따라서 5 개의 다른 classifiers를 호출하려면 동일한 메소드를 5 번 작성할 필요가 없으므로 불필요한 중복을 제거해 줍니다."},{"metadata":{"_uuid":"04d921ea89a0560cf010e956e4065bb2eaf21619","_cell_guid":"c017c078-172d-16e9-65f2-4a01c6e0626f","trusted":true},"cell_type":"code","source":"# 나중에 유용하게 쓸 수있는 유용한 매개 변수\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # 재현성을 위한 값\nNFOLDS = 5 # out-of-fold prediction 위한 fold값 설정\nkf = KFold(5, shuffle=True, random_state=0)\n\n# Sklearn classifier 확장하기 위한 클래스\n#**params https://stackoverflow.com/questions/36901/what-does-double-star-asterisk-and-star-asterisk-do-for-parameters\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params) \n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"634199a5b36cba6b4d85db2c0401426a58b95279","_cell_guid":"75f59fa7-65a4-e53e-95df-4c747a608408"},"cell_type":"markdown","source":"\n파이썬에서 클래스나 객체를 만들어 보지 않은 사람들을 위해 이미 이것을 알고 있는 사람들은 조금만 참아주시기 바랍니다. 위에서 주어진 코드가 무엇을하는지 설명하겠습니다. 기본 classifiers 만들때 Sklearn 라이브러리에 이미있는 모델만 사용하므로 클래스를 확장 할 수 있습니다.\n\n**def init** : 클래스의 기본 생성자를 호출하는 Python 표준입니다. 즉, 객체(classifier)를 만들려면 clf(원하는 sklearn classifier), seed(임의 시드) 및 params(parameters for the classifiers)의 매개 변수값을 지정해야합니다.\n\n나머지 코드는 단순히 sklearn classifier안에 이미 존재하는 해당 메소드를 호출하는 클래스의 메소드입니다. 기본적으로 다양한 Sklearn 분류기를 확장하는 wrapper 클래스를 만들어 stacker에 여러 learners를 구현할 때 동일한 코드를 반복 작성해야하는 부담을 줄여줍니다."},{"metadata":{"_uuid":"4d193c581df258e823aff2796bf015cf906aac99","_cell_guid":"6f67620d-b531-a2fa-c297-e951970c3c28"},"cell_type":"markdown","source":"### Out-of-Fold Predictions\n\n이제 소개 섹션에서 언급 한 것처럼 스태킹은 기본 classifiers의 예측을 두 번째 레벨 모델에 대한 학습용 input값으로 사용합니다. 그러나 전체 학습 데이터에대해 기본 모델을 단순히 학습 할 수는 없습니다. 전체 테스트 세트에서 예측을 생성 한 뒤에 두 번째 레벨 모델의 학습을 위해 이 예측값을 입력해야 합니다. 이렇게하면 기본 모델 예측이 이미 테스트 데이터 셋을 보았다는 risk를 수반하므로 이 예측값을 feeding할 때 overfitting이 발생합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.empty((5, 10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46a93dc062e973832cecd50246d0d7581aafb02b","_cell_guid":"406d0494-1d0c-3126-19d9-bc53127c4249","trusted":true},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test) # i번째 fold 의 예측값\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b1a7767ae61b6b217a3311e89190b05ab0a4891","_cell_guid":"3cd92196-f7ba-4f14-0fc4-36520fbcb2ca"},"cell_type":"markdown","source":"# Generating our Base First-Level Models \n\n자 이제 우리의 첫번째 단계의 classification으로 5개의 learning model을 준비합시다. 이 models은 Sklean library를 통해 쉽게 불러올 수 있습니다. 모델의 리스트는 다음과 같습니다:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine"},{"metadata":{"_uuid":"12e3a5f76fb118ff6906431fc60e7010e33106ad","_cell_guid":"0ef6862a-b5cc-6829-f040-d2b2b2c817f3"},"cell_type":"markdown","source":"**Parameters**\n\n완전성을 위해 여기에 나열 할 매개 변수에 대한 간단한 요약만 제공합니다.\n\n**n_jobs** : 학습과정에 사용되는 cores의 수로 만약 -1로 설정되어 있다면 모든 core가 사용됩니다.\n\n**n_estimators** : 당신의 learning model 안에 classification의 수(set to 10 per default)\n\n**max_depth** : 트리의 깊이의 최대값 또는 얼마나 많은 노드를 확장할지에 대한 값으로 이것이 너무 큰 값을 가진다면 트리가 지나치게 깊게 자랄 수 있으므로 오버피팅의 위험을 수반할 것입니다.\n\n**verbose** : 학습 과정 중에 텍스트를 출력할지 여부를 조정합니다. 값 0은 모든 텍스트를 억제하는 반면 값 3은 모든 반복에서 트리 학습 프로세스를 출력합니다.\n\n\nSklearn 공식 웹 사이트를 통해 자세한 설명을 확인하십시오. 거기서 모델에서 설정할 수 있는 다른 유용한 매개 변수의 전체 정보를 찾을 수 있을것입니다.\n"},{"metadata":{"_uuid":"d77772886c0125e022d1fbb39cd484c95121d74d","_cell_guid":"6e634aba-90b6-0620-eceb-3e1a39fbfedc","trusted":true},"cell_type":"code","source":"# 언급한 classifier의 parameters를 넣어준다\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a49a6c0cbe7028035b6efb7f9374084f3fa152a","_cell_guid":"ee6325d7-a7c1-c767-fcba-4c59eaa83787"},"cell_type":"markdown","source":"또한, OOP 프레임 워크 내에서 객체와 클래스에 대해 언급 한 이후로, 앞서 정의한 Helper Sklearn 클래스를 통해 5 가지 학습 모델을 나타내는 5 개의 객체를 생성해 보겠습니다."},{"metadata":{"_uuid":"3bd54b4dfbbedc86d6c415cf83936bb8cd6c0973","_cell_guid":"6798243f-5ff9-527b-01b1-09cfe62284bb","trusted":true},"cell_type":"code","source":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd3f527543e61b3841f242b744c3c5d9af608941","_cell_guid":"46e1e6fc-92e2-e7e2-1ab9-470e6c7039ce"},"cell_type":"markdown","source":"**train and test sets으로 부터 Numpy arrays를 생성**\n\n첫 번째 레이어 base 모델을 준비한 다음에는 다음과 같이 원래 데이터 프레임에서 NumPy 배열을 생성하여 classifier의 input 에 대한 train 및 test 데이터를 준비 할 수 있습니다."},{"metadata":{"_uuid":"95fdd1e8cd9f23cfcef3bed92511da084a323c55","_cell_guid":"968cbd7f-80b2-7f8d-2ad6-b68b3aeae671","trusted":true},"cell_type":"code","source":"# 우리의 모델로 값을 넣어주기 위한 train, test and target ( Survived) dataframes의 Numpy arrays 생성\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d30ac9528b69cdaec565b647fee98a92391112c9","_cell_guid":"606b619c-2301-8aaa-a5de-781d981c4a6f"},"cell_type":"markdown","source":"**첫번째 수준의 예측의 값** \n\n\n이제 5 개의 기본 classifier에 train and test data를 제공하고 앞서 정의한 Out-of-Fold 예측 함수를 사용하여 첫 번째 수준 예측을 생성합니다. 아래 코드를 실행하려면 몇 분 정도 기다려주십시오."},{"metadata":{"_uuid":"114750e2d5e4fdd234ccd8647fc349463a56fa09","_cell_guid":"79bd2a86-82e2-648a-e816-9660e89794ad","trusted":true},"cell_type":"code","source":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cf41b3d9a541c9d39b645a66c8f1116eaf76861","_cell_guid":"3f292e65-fe8a-d662-6ace-41a19866d671"},"cell_type":"markdown","source":"**다른 classifiers에서 만들어진 Feature importance **\n\n이제 첫 번째 수준의 classifiers를 학습시켰으므로, Sklearn 모델의 매우 멋진 기능을 활용할 수 있습니다. 이는 매우 간단한 코드 한 줄로 실행되어서 다양한 feature의 importance의 값을 출력해줍니다. \n\nSklearn 문서에 따르면 대부분의 classifier에는 단순히 **.feature_importances**를 입력하여 feature importance를 반환해주는 특성이 내제되어 있습니다. 그러므로 우리는 이 매우 유용한 특성을 앞서 언급한 함수를 통해 호출할 것이고 feature importance를 다음과 같이 그래프로 나타낼 것입니다."},{"metadata":{"_uuid":"b3b0356c8bef0dceb5fcfa7fb7a11359010b2098","_cell_guid":"ed9cf8b5-95a4-d974-fb11-592214949d1f","trusted":true},"cell_type":"code","source":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0c66aa58b45b917157f47c811e7d222513ddb0d","_cell_guid":"f1be8fbb-34c0-3f92-c7d2-757fceced507"},"cell_type":"markdown","source":"feature importance값을 어떻게 할당하고 저장해야 하는지 아직 정확하게 파악하지 못했습니다. 그러므로 코드로 값을 출력하고 그 다음 아래와 같은 파이썬 리스트에 붙여넣기 하겠습니다. (sorry for the lousy hack)"},{"metadata":{"_uuid":"527425374ec8cd66edc015842f352a95041821c6","_cell_guid":"bde9b5fd-3100-8f21-053e-45015b99cf65","trusted":true},"cell_type":"code","source":"rf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.07462718,  0.4593142 ,  0.01340093]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d59169f89c5541177f537200a90805420b449001","_cell_guid":"2a03ad8f-0ea1-5afa-a6e8-56284482c646"},"cell_type":"markdown","source":"Plotly 패키지를 통해 쉽게 그래프를 그릴 수 있도록 feature importance data가 포함된 list로 데이터 프레임을 만듭니다."},{"metadata":{"_uuid":"6f68b3033a8f185f61d83e80323c2486024f5d4d","_cell_guid":"635a063f-281d-66d4-6572-587ebecd6b4b","trusted":true},"cell_type":"code","source":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e25675f239b0ab008e0264917abff497795681a","_cell_guid":"06b9f410-f93e-0206-b029-24df035eea2b"},"cell_type":"markdown","source":"**Plotly scatterplots통해 대화형 feature importance plotting**\n\n이 시점에서 대화형 Plotly 패키지를 사용해서 plotly scatter plot 통해 \"Scatter\"을 호출하여 다른 classifier들의 feature importance를 시각화 하겠습니다  "},{"metadata":{"_uuid":"d8ee9114cd391433835f1272ef81d0a729c78b71","_cell_guid":"1ac351c6-83c6-c35b-9d66-64f16b5d073f","trusted":true},"cell_type":"code","source":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74","_cell_guid":"553828f0-c994-5ee1-695f-9373f11a1a7b"},"cell_type":"markdown","source":"\n이제 모든 featuer importance의 평균을 계산하고 데이터 프레임의 새로운 열로 그 값을 저장할것 입니다."},{"metadata":{"_uuid":"f611812e2c9de3773df2264dfb2b13c0995807ac","_cell_guid":"06847850-a829-0858-b12c-7b66e53e030a","trusted":true},"cell_type":"code","source":"# 평균 값을 포함하고 있는 새로운 column 생성\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f0aff0896fef90b326fff7816393fef0e0cb992","_cell_guid":"5645e647-c517-7822-f881-b8d7e38ef5da"},"cell_type":"markdown","source":"**Feature Importances 평균의 Plotly Barplot**\n\n우리의 모든 classifier에 대한 feature importance 평균 값을 얻었으므로 그 값들을 이용해 Plotly bar plot을 다음과 같이 그릴 수 있다."},{"metadata":{"_uuid":"0bd069388b419fe45306c01825aa3e6f5466ba2b","_cell_guid":"63d86121-8c29-4b7f-b2ad-12b0a593f1d6","trusted":true},"cell_type":"code","source":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbb76d189e8d03921caaacfa9545cef894348c7d","_cell_guid":"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0"},"cell_type":"markdown","source":"# Second-Level Predictions from the First-level Output"},{"metadata":{"_uuid":"fed132782b73dda8d265065867e7f57c0aed7f50","_cell_guid":"6b901750-ccdd-38ca-d8ea-1c361121ec4f"},"cell_type":"markdown","source":"**First-level output as new features**\n\n첫번째 수준의 예측값을 얻은 후에 다음 classifier를 위한 training data로 사용될 feature의 새로운 set을 만드는것을 생각 할 수 있다. 아래 코드에 따라 이전 classifier로부터 얻은 첫 번째 수준의 예측값을 새로운 열로 가져오고 예측값를 이용해 다음의 classifier를 학습시킵니다."},{"metadata":{"_uuid":"a5945e93337b87a1a8ee5580856768bbb14c07cd","_cell_guid":"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586","trusted":true},"cell_type":"code","source":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200","_cell_guid":"f69c11db-d84e-8536-4c7e-382fbe67483e"},"cell_type":"markdown","source":"**Correlation Heatmap of the Second Level Training set**"},{"metadata":{"_uuid":"9714ecaedf7385c5b8ad346ab909215eb9f2abc6","_cell_guid":"4cf590ee-133f-6487-cf5a-53f346893d1c","trusted":true},"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9ef0298b568e43da6925f385403e0d77bd6e33","_cell_guid":"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6"},"cell_type":"markdown","source":"서로 상관성이 낮은 학습된 모델을 가지는것이 더 좋은 score를 생산한다는 장점이 있다는 기사와 Kaggle 경쟁 우승자 이야기가 꽤 많이 있습니다."},{"metadata":{"_uuid":"fef365199854ca3fff754399b4699d941b7e43b8","_cell_guid":"6685fa11-497f-3fc2-ab1f-97f92d6eca61","trusted":true},"cell_type":"code","source":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c","_cell_guid":"a02a94ab-3c9c-a824-7168-e964c5a0f5d5"},"cell_type":"markdown","source":"이제 첫번째 수준의 train과 test 예측을 x_train 과 x_test로 연결시키고 결합 했으므로 두 번째 수준의 학습 모델을 적용 할 수 있습니다."},{"metadata":{"_uuid":"dc4a32e9a8e7c9e611124cba676e5d28240b38be","_cell_guid":"628a03ea-933c-7075-a589-0ff7af237dfd"},"cell_type":"markdown","source":"### XGBoost를 통한 두번째 수준의 학습 모델\n\n여기서는 boosted tree learning model으로 매우 유명한 XGBoost를 선택했습니다. 이건 큰 scale의 boosted tree algorithms를 최적화 하도록 설계되어 있습니다. 알고리즘에 대한 정보를 얻고 싶다면 여기를 확인하세요.\n[official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\n어쨋든 우리는 XGBclassifier를 호출하여 이것을 첫번째 수준의 train과 target에 fit 하고 학습된 모델을 test data를 예측하기 위해 다음과 같이 사용할 것입니다.:"},{"metadata":{"_uuid":"5155d370069fe6de0fe5105309342ce55130dae8","_cell_guid":"3a7c7517-b9a3-3a21-3a7b-299ca37c6843","trusted":true},"cell_type":"code","source":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0101e6b843f6378838874ccfb844ed464b81d627","_cell_guid":"0a8152d8-6842-ed00-6bc5-47a511adce1c"},"cell_type":"markdown","source":"model에서 사용되는 XGBoost parameters를 빠르게 훑어보자.:\n\n**max_depth** :  당신의 tree가 어느정도의 깊이로 자라길 원하는가. 만약 너무 큰 값을 가진다면 overfitting의 가능성을 수반한다.\n\n**gamma** : 트리의 리프 노드에서 추가 파티션을 작성하는 데 필요한 최소 손실 감소.크면 클수록 보수적인 알고리즘이 될 것입니다\n\n**eta** : 오버 핏팅을 방지하기 위해 각 부스팅 단계에서 사용되는 축소되는 크기"},{"metadata":{"_uuid":"52ac0cd99cee0099d86a180127da42ff7fff960a","_cell_guid":"6b4a5c81-e968-d41e-27e4-871481019867"},"cell_type":"markdown","source":"**Producing the Submission file**\n\n마침내 우리의 모든 첫번째 수준과 두번째 모델을 학습시키고 fit 했다. 이제 우리는 Titanic competition에 submission을 위해 적절한 형식으로 예측값을 다음과 같이 출력할 수 있다"},{"metadata":{"collapsed":true,"_uuid":"9d607d829dbadd6c72ee01c9735a642435eb53e6","_cell_guid":"f5a31787-5fe1-a559-bee9-ad6b6d83ae14","trusted":true},"cell_type":"code","source":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5","_cell_guid":"1e56c738-b8f3-95e4-d642-c483f9757ed8"},"cell_type":"markdown","source":"**개선을 위한 단계**\n\n마지막으로, 위의 단계는 앙상블 스태커(ensemble stacker)를 제작하는 아주 간단한 방법을 보여줍니다. 당신은 아마도 가장 높은 수준의 Kaggle 대회에서의 앙상블은 괴상한 stacked classifiers의 조합은 물론이거니와 2번째 수준 이상의 stacking 수준도 보여준다는걸 확인 할 수 있을것 입니다.\n\n점수를 높이기 위해 취할 수있는 추가 단계는:\n\n 1. 최적의 parameter 값을 찾기 위해 모델을 학습 할 때 좋은 cross-validation strate 전략을 구현합니다.\n 2. 학습을위한 다양한 base 모델을 사용하십시오. 결과가 더 관련성이 없으면 좋을수록 최종 점수가 높아집니다."},{"metadata":{"_uuid":"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e","_cell_guid":"9a8f83fd-d0e8-035a-cf7f-25c9012e9373"},"cell_type":"markdown","source":"### Conclusion\n\n이 notebook이 학습 모델을 스태킹하기위한 working script로 시작하는데 조금이라도 도움이 되도록 작성했습니다. 다시 한번 Faron과 Sina에게 감사의 마음을 전합니다.\n\n스태킹이나 앙상블에 관한 다른 훌륭한 자료는 MLWave 웹 사이트에서 기사를 읽어볼 수 있습니다.: [Kaggle Ensembling Guide][1]. \n\n그럼 다음에 봐요, 안녕\n  [1]: http://mlwave.com/kaggle-ensembling-guide/"},{"metadata":{"collapsed":true,"_uuid":"8549710f41f2ca55f11400e7914ed2f15c380964","_cell_guid":"74641376-e124-e46f-0e7c-8cbee2a1321b","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_change_revision":0,"_is_fork":false,"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}